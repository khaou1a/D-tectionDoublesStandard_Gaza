{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "239d4a6b",
   "metadata": {},
   "source": [
    "# pretraining \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "124efde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Pr√©traitement Expert en cours : GAZA...\n",
      "‚úÖ Termin√© ! GAZA sauvegard√© dans corpus/corpus_gaza_pretraiter.json\n",
      "\n",
      "üöÄ Pr√©traitement Expert en cours : UKRAINE...\n",
      "‚úÖ Termin√© ! UKRAINE sauvegard√© dans corpus/corpus_ukraine_pretraiter.json\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# SCRIPT DE PR√âTRAITEMENT MULTI-VUES - VERSION EXPERT CORRIG√âE\n",
    "# =================================================================\n",
    "# Auteur : Expert NLP\n",
    "# But : Segmentation par phrases, conservation des dates/chiffres \n",
    "# et cr√©ation des vues : Lexicale, Structurelle (S√©mantique) et Sentiment.\n",
    "# =================================================================\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# --- T√©l√©chargement des ressources NLTK n√©cessaires ---\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\" Convertit les tags POS de Treebank pour WordNet (Crucial pour Lab 6) \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def setup_selective_stopwords():\n",
    "    \"\"\" \n",
    "    Garde les mots cl√©s de la voix passive (by, was, were) \n",
    "    et les n√©gations (not, no) pour l'analyse s√©mantique et de sentiment.\n",
    "    \"\"\"\n",
    "    base_stop = set(stopwords.words('english'))\n",
    "    preserve = {'not', 'no', 'never', 'by', 'was', 'were', 'been', 'is', 'are', 'against'}\n",
    "    return base_stop - preserve\n",
    "\n",
    "# Initialisation des outils\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "custom_stopwords = setup_selective_stopwords()\n",
    "standard_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "\n",
    "# --- FONCTION UTILITAIRE DE R√âPARATION ---\n",
    "def reparer_mots_colles(text):\n",
    "    \"\"\"\n",
    "    S√©pare les mots coll√©s par erreur lors du scraping (ex: 'LadyMelania' -> 'Lady Melania').\n",
    "    Doit √™tre appliqu√© AVANT la mise en minuscule.\n",
    "    \"\"\"\n",
    "    if not text: return \"\"\n",
    "    \n",
    "    # Cas 1 : CamelCase (LadyMelania -> Lady Melania)\n",
    "    # On cherche une minuscule suivie imm√©diatement d'une majuscule\n",
    "    text = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', text)\n",
    "    \n",
    "    # Cas 2 : Ponctuation coll√©e (End.Start -> End. Start)\n",
    "    # On cherche un point suivi d'une majuscule sans espace\n",
    "    text = re.sub(r'(?<=\\.)(?=[A-Z])', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# --- LES TROIS PIPELINES DE PR√âTRAITEMENT ---\n",
    "def pipeline_lexical(text):\n",
    "    \"\"\" \n",
    "    VERSION 1 : Pour TF-IDF et Nuages de mots (Lab 3, 7).\n",
    "    Filtre les stopwords et la ponctuation, mais garde les chiffres significatifs.\n",
    "    \"\"\"\n",
    "    # 1. D'abord on r√©pare les collages (tant qu'il y a des majuscules)\n",
    "    text = reparer_mots_colles(text)\n",
    "    \n",
    "    # Tokenization standard\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Nettoyage : on garde les mots alphanum√©riques (mots + nombres)\n",
    "    clean_tokens = [t for t in tokens if t.isalnum() and t not in standard_stopwords and len(t) > 1]\n",
    "    # Lemmatisation\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in clean_tokens]\n",
    "    return lemmas\n",
    "\n",
    "def pipeline_structural_semantique(text):\n",
    "    \"\"\" \n",
    "    VERSION 2 : Analyse S√©mantique / N-Grams .\n",
    "    Structure : Liste de phrases -> Chaque phrase contient des dictionnaires (Mot, POS, Lemme).\n",
    "    Indispensable pour l'analyse de la responsabilit√© (Voix passive/active).\n",
    "    \"\"\"\n",
    "# 1. R√©paration initiale\n",
    "    text = reparer_mots_colles(text)\n",
    "\n",
    "    # 1. Segmentation en phrases (sur texte propre avec espaces corrig√©s)\n",
    "    sentences = sent_tokenize(text)\n",
    "    processed_sentences = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        # 2. Tokenization par phrase\n",
    "        tokens = word_tokenize(sent)\n",
    "        # 3. √âtiquetage grammatical (POS Tagging)\n",
    "        tagged = pos_tag(tokens)\n",
    "        \n",
    "        sentence_data = []\n",
    "        for word, tag in tagged:\n",
    "            low_word = word.lower()\n",
    "            # On ne filtre QUE les vrais stopwords (pas ceux de structure/voix passive)\n",
    "            if low_word not in custom_stopwords:\n",
    "                # Lemmatisation intelligente bas√©e sur le POS\n",
    "                lemma = lemmatizer.lemmatize(low_word, get_wordnet_pos(tag))\n",
    "                sentence_data.append({\"w\": word, \"t\": tag, \"l\": lemma})\n",
    "        \n",
    "        if sentence_data: # On n'ajoute pas les phrases vides\n",
    "            processed_sentences.append(sentence_data)\n",
    "            \n",
    "    return processed_sentences\n",
    "\n",
    "def pipeline_sentiment(text):\n",
    "    \"\"\" \n",
    "    VERSION 3 : Pour Analyse de Polarit√© (Lab 9, 10).\n",
    "    Conserve les n√©gations pour √©viter d'inverser le sens √©motionnel.\n",
    "\n",
    "    \"\"\"\n",
    "    # 1. R√©paration\n",
    "    text = reparer_mots_colles(text)\n",
    "    # 2. Tokenization\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # On filtre les stopwords mais on pr√©serve 'not', 'no', 'never'\n",
    "    clean_tokens = [t for t in tokens if t.isalnum() and (t not in standard_stopwords or t in {'not', 'no', 'never'})]\n",
    "    return clean_tokens\n",
    "\n",
    "# --- FONCTION PRINCIPALE D'EX√âCUTION ---\n",
    "\n",
    "def traiter_corpus(filename_in, filename_out, conflict_label):\n",
    "    print(f\"\\nüöÄ Pr√©traitement Expert en cours : {conflict_label}...\")\n",
    "    \n",
    "    if not os.path.exists(filename_in):\n",
    "        print(f\"‚ùå Erreur : {filename_in} introuvable.\")\n",
    "        return\n",
    "\n",
    "    with open(filename_in, 'r', encoding='utf-8') as f:\n",
    "        articles = json.load(f)\n",
    "\n",
    "    corpus_pretraite = []\n",
    "\n",
    "    for art in articles:\n",
    "        content = art['content']\n",
    "        \n",
    "        # Cr√©ation des 3 vues pour l'article\n",
    "        processed_data = {\n",
    "            \"title\": art.get('title', 'N/A'),\n",
    "            \"conflict\": conflict_label,\n",
    "            \"scraped_at\": art.get('scraped_at', ''),\n",
    "            # Vue 1 : Mots-cl√©s (Lab 7)\n",
    "            \"lexical_view\": pipeline_lexical(content),\n",
    "            # Vue 2 : Syntaxe et S√©mantique par phrase (Lab 6, 8, 10)\n",
    "            \"structural_view\": pipeline_structural_semantique(content),\n",
    "            # Vue 3 : Sentiments avec n√©gations (Lab 9)\n",
    "            \"sentiment_view\": pipeline_sentiment(content)\n",
    "        }\n",
    "        corpus_pretraite.append(processed_data)\n",
    "\n",
    "    # Sauvegarde\n",
    "    with open(filename_out, 'w', encoding='utf-8') as f:\n",
    "        json.dump(corpus_pretraite, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"‚úÖ Termin√© ! {conflict_label} sauvegard√© dans {filename_out}\")\n",
    "\n",
    "# --- LANCEMENT ---\n",
    "traiter_corpus('corpus/corpus_palestine_nettoye_v1.json', 'corpus/corpus_gaza_pretraiter.json', 'GAZA')\n",
    "traiter_corpus('corpus/corpus_ukraine_nettoye_v1.json', 'corpus/corpus_ukraine_pretraiter.json', 'UKRAINE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ce7a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Pr√©traitement Expert en cours : GAZA...\n",
      "‚úÖ Termin√© ! GAZA sauvegard√© dans corpus/corpus_gaza_pretraiter.json\n",
      "\n",
      "üöÄ Pr√©traitement Expert en cours : UKRAINE...\n",
      "‚úÖ Termin√© ! UKRAINE sauvegard√© dans corpus/corpus_ukraine_pretraiter.json\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# SCRIPT DE PR√âTRAITEMENT MULTI-VUES - VERSION EXPERT (CORRIG√âE)\n",
    "# =================================================================\n",
    "# Auteur : Expert NLP\n",
    "# Am√©lioration : D√©collage automatique des mots (TrumpAnnounced -> Trump Announced)\n",
    "# =================================================================\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# --- T√©l√©chargement des ressources ---\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# --- CONFIGURATION LINGUISTIQUE ---\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'): return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'): return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'): return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'): return wordnet.ADV\n",
    "    else: return wordnet.NOUN\n",
    "\n",
    "def setup_selective_stopwords():\n",
    "    base_stop = set(stopwords.words('english'))\n",
    "    preserve = {'not', 'no', 'never', 'by', 'was', 'were', 'been', 'is', 'are', 'against'}\n",
    "    return base_stop - preserve\n",
    "\n",
    "# Outils\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "custom_stopwords = setup_selective_stopwords()\n",
    "standard_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# --- FONCTION DE R√âPARATION ---\n",
    "def reparer_mots_colles(text):\n",
    "    \"\"\"\n",
    "    S√©pare les mots coll√©s par erreur de scraping AVANT la mise en minuscule.\n",
    "    Int√®gre une logique Regex + un Dictionnaire de Patchs pour les cas r√©sistants.\n",
    "    \"\"\"\n",
    "    if not text: return \"\"\n",
    "    \n",
    "    # 1. LISTE DE PATCHS SP√âCIFIQUES (Correction manuelle des erreurs r√©currentes)\n",
    "    # C'est ici qu'on force la correction de \"Trumpannounced\" et autres erreurs identifi√©es\n",
    "    patchs = {\n",
    "        \"Trumpannounced\": \"Trump announced\",\n",
    "        \"trumpannounced\": \"trump announced\",\n",
    "        \"LadyMelania\": \"Lady Melania\",\n",
    "        \"Ladymelania\": \"Lady Melania\",\n",
    "        \"whitehouse\": \"white house\",\n",
    "        \"WhiteHouse\": \"White House\"\n",
    "    }\n",
    "    \n",
    "    # On applique les patchs d'abord\n",
    "    for erreur, correction in patchs.items():\n",
    "        text = text.replace(erreur, correction)\n",
    "    \n",
    "    # 2. D√©coller le CamelCase g√©n√©rique (minuscule suivie d'une Majuscule)\n",
    "    # Ex: TrumpSaid -> Trump Said\n",
    "    text = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', text)\n",
    "    \n",
    "    # 3. D√©coller la ponctuation manquante\n",
    "    # Ex: word.Next -> word. Next\n",
    "    text = re.sub(r'(?<=[a-z])\\.(?=[A-Z])', '. ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# --- LES 3 PIPELINES (AVEC R√âPARATION INT√âGR√âE) ---\n",
    "\n",
    "def pipeline_lexical(text):\n",
    "    \"\"\" VERSION 1 : Lexicale (TF-IDF) \"\"\"\n",
    "    # √âTAPE CRUCIALE : On r√©pare D'ABORD, quand les majuscules existent encore\n",
    "    text = reparer_mots_colles(text)\n",
    "    \n",
    "    # Ensuite on met en minuscule\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Filtrage\n",
    "    clean_tokens = [t for t in tokens if t.isalnum() and t not in standard_stopwords and len(t) > 1]\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in clean_tokens]\n",
    "    return lemmas\n",
    "\n",
    "def pipeline_structural_semantique(text):\n",
    "    \"\"\" VERSION 2 : S√©mantique (Phrases & POS) \"\"\"\n",
    "    # √âTAPE CRUCIALE : R√©paration initiale\n",
    "    text = reparer_mots_colles(text)\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    processed_sentences = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        tokens = word_tokenize(sent)\n",
    "        tagged = pos_tag(tokens)\n",
    "        \n",
    "        sentence_data = []\n",
    "        for word, tag in tagged:\n",
    "            low_word = word.lower()\n",
    "            if low_word not in custom_stopwords:\n",
    "                lemma = lemmatizer.lemmatize(low_word, get_wordnet_pos(tag))\n",
    "                sentence_data.append({\"w\": word, \"t\": tag, \"l\": lemma})\n",
    "        \n",
    "        if sentence_data:\n",
    "            processed_sentences.append(sentence_data)\n",
    "            \n",
    "    return processed_sentences\n",
    "\n",
    "def pipeline_sentiment(text):\n",
    "    \"\"\" VERSION 3 : Sentiment (Avec N√©gations) \"\"\"\n",
    "    # √âTAPE CRUCIALE : R√©paration initiale\n",
    "    text = reparer_mots_colles(text)\n",
    "    \n",
    "    tokens = word_tokenize(text.lower())\n",
    "    clean_tokens = [t for t in tokens if t.isalnum() and (t not in standard_stopwords or t in {'not', 'no', 'never'})]\n",
    "    return clean_tokens\n",
    "\n",
    "# --- EX√âCUTION ---\n",
    "\n",
    "def traiter_corpus(filename_in, filename_out, conflict_label):\n",
    "    print(f\"\\nüöÄ Pr√©traitement Expert en cours : {conflict_label}...\")\n",
    "    \n",
    "    if not os.path.exists(filename_in):\n",
    "        print(f\"‚ùå Erreur : {filename_in} introuvable.\")\n",
    "        return\n",
    "\n",
    "    with open(filename_in, 'r', encoding='utf-8') as f:\n",
    "        articles = json.load(f)\n",
    "\n",
    "    corpus_pretraite = []\n",
    "\n",
    "    for art in articles:\n",
    "        content = art['content']\n",
    "        \n",
    "        processed_data = {\n",
    "            \"title\": art.get('title', 'N/A'),\n",
    "            \"conflict\": conflict_label,\n",
    "            \"scraped_at\": art.get('scraped_at', ''),\n",
    "            \"lexical_view\": pipeline_lexical(content),\n",
    "            \"structural_view\": pipeline_structural_semantique(content),\n",
    "            \"sentiment_view\": pipeline_sentiment(content)\n",
    "        }\n",
    "        corpus_pretraite.append(processed_data)\n",
    "\n",
    "    with open(filename_out, 'w', encoding='utf-8') as f:\n",
    "        json.dump(corpus_pretraite, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"‚úÖ Termin√© ! {conflict_label} sauvegard√© dans {filename_out}\")\n",
    "\n",
    "# --- LANCEMENT ---\n",
    "# Assurez-vous d'utiliser les fichiers NETTOY√âS (v1) comme source, car ils contiennent encore les majuscules !\n",
    "traiter_corpus('corpus/corpus_palestine_nettoye_v1.json', 'corpus/corpus_gaza_pretraiter.json', 'GAZA')\n",
    "traiter_corpus('corpus/corpus_ukraine_nettoye_v1.json', 'corpus /corpus_ukraine_pretraiter.json', 'UKRAINE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
