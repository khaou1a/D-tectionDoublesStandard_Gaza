{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a8075b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Début collecte corpus Ukraine ---\n",
      "Succès : Melania Trump says 7 more Ukrainian children reunited with f...\n",
      "Succès : Ukraine’s 'Underground Railroad' rescues abducted Ukrainian ...\n",
      "Succès : Humanitarian aid expert reveals dramatic rescue of children ...\n",
      "Succès : Half a million children become refugees as Russia-Ukraine wa...\n",
      "Succès : Ukraine's refugee children: Amid the war, how to help kids s...\n",
      "Succès : 'Thousands' of Ukrainian children forcibly deported to Russi...\n",
      "Succès : Ukrainian child death toll mounts, humanitarian crisis worse...\n",
      "Succès : 'Just evil': Top Republican details Russia's 'horrific' mass...\n",
      "Succès : Trump administration ends program to track kidnapped Ukraini...\n",
      "Succès : SENS KLOBUCHAR AND GRASSLEY: America can't ignore Russia kid...\n",
      "Succès : Rescuers continue the search at Kyiv children's hospital hit...\n",
      "Succès : Children traumatized by war in Ukraine find mentors from une...\n",
      "Succès : Ukrainian children battling cancer are evacuated to Poland d...\n",
      "Succès : Ukraine's top prosecutor speaks of 'evil' Russian atrocities...\n",
      "Succès : Half of Ukraine's children displaced: 'A grim milestone'...\n",
      "Succès : Tennessee becomes first state to take in young Ukrainian hos...\n",
      "Succès : Premature babies in Ukraine rescued via ambulance escort dur...\n",
      "Succès : Jessica Chastain shares footage from 'life-changing' Ukraine...\n",
      "Succès : Ukrainian mother fleeing country with her son details escape...\n",
      "Succès : Rev. Franklin Graham on Ukraine war: 'Children always suffer...\n",
      "Succès : Ukraine's foreign minister accuses Russia of 'war crimes' wi...\n",
      "Succès : Rescuers continue the search at Kyiv children's hospital hit...\n",
      "Succès : World leaders react to Bucha, Ukraine massacre: ‘Genocide’, ...\n",
      "Succès : Strikes on Ukraine hospital kill 2-day-old baby, officials s...\n",
      "Succès : Ukrainian officials say 300 dead in Russian airstrikes on Ma...\n",
      "Succès : 85 children killed in Ukraine, more than 100 wounded since s...\n",
      "Succès : Ukraine refugee numbers reach 2.5 million, many children, UN...\n",
      "Succès : Nearly 50 children from Russian-occupied regions in Ukraine ...\n",
      "Succès : Ukraine rescue team brings back 31 children from Russia amid...\n",
      "Succès : Russian attacks on Ukrainian hospitals 'could be war crimes,...\n",
      "Succès : Russia denies it bombed Ukrainian maternity hospital: 'fake ...\n",
      "Succès : Ukrainian lawmaker says Russia forces trying to forcibly dep...\n",
      "Succès : New clothing to be delivered to Ukraine children...\n",
      "Succès : Bomb blast in Moscow kills two police officers, days after a...\n",
      "Succès : ‘Massive’ Russian attack on Ukraine kills 4-year-old child, ...\n",
      "Succès : Ukraine presents response to US peace plan as Trump says Eur...\n",
      "Succès : Putin says sanctions over Ukraine are like a declaration of ...\n",
      "Succès : Russia accused of using hunger as a weapon of war in Ukraine...\n",
      "Succès : Civilians flee Sumy after evacuation corridor opens, as 2 mi...\n",
      "Succès : Ukrainians who fled to UK being refused asylum on grounds it...\n",
      "\n",
      "--- TERMINÉ ---\n",
      "40 articles sauvegardés dans corpus_ukraine.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util import Retry\n",
    "import re\n",
    "\n",
    "# ============================\n",
    "# 1. URLs UKRAINE\n",
    "# ============================]\n",
    "urls_ukraine = [\n",
    "    \n",
    "    \"https://www.foxnews.com/politics/melania-trump-says-7-more-ukrainian-children-reunited-families-part-initiative-russia\",\n",
    "    \"https://www.foxnews.com/world/ukraines-underground-railroad-rescues-abducted-ukrainian-children-from-russian-reeducation-camps\",\n",
    "    \"https://www.foxnews.com/lifestyle/humanitarian-aid-expert-rescue-children-ukraine\",\n",
    "    \"https://www.foxnews.com/lifestyle/unicef-gives-aid-russia-ukraine-refugees\",\n",
    "    \"https://www.foxnews.com/lifestyle/ukraine-refugee-children-war-help-kids\",\n",
    "    \"https://www.foxnews.com/world/thousands-ukrainian-children-forcibly-deported-russia-july-us-security-official-says\",\n",
    "    \"https://www.foxnews.com/world/ukrainian-children-death-toll-humanitarian-crisis-food\",\n",
    "    \"https://www.foxnews.com/politics/just-evil-top-republican-details-russias-horrific-mass-abductions-ukrainian-children\",\n",
    "    \"https://www.foxnews.com/politics/trump-administration-ends-program-track-kidnapped-ukrainian-children-russia-lawmakers-say\",\n",
    "    \"https://www.foxnews.com/opinion/sens-klobuchar-grassley-america-cant-ignore-russia-kidnapping-ukrainian-children\",\n",
    "    \"https://www.foxnews.com/world/rescuers-continue-search-kyiv-childrens-hospital-hit-russian-missile\",\n",
    "    \"https://www.foxnews.com/lifestyle/children-traumatized-war-ukraine-mentors-places\",\n",
    "    \"https://www.foxnews.com/lifestyle/ukrainian-children-cancer-evacuated-poland-war\",\n",
    "    \"https://www.foxnews.com/world/ukraines-top-prosecutor-speaks-evil-russian-atrocities\",\n",
    "    \"https://www.foxnews.com/world/half-ukraine-children-displaced\",\n",
    "    \"https://www.foxnews.com/us/tennessee-first-state-receive-child-cancer-patients-ukraine\",\n",
    "    \"https://www.foxnews.com/lifestyle/premature-babies-ukraine-rescued-ambulance-kyiv-shelling\",\n",
    "    \"https://www.foxnews.com/entertainment/jessica-chastain-shares-footage-life-changing-ukraine-trip-displaced-children-destruction\",\n",
    "    \"https://www.foxnews.com/media/ukrainian-mother-fleeing-country-with-her-son-details-escape-i-wish-i-never-experienced-that\",\n",
    "    \"https://www.foxnews.com/lifestyle/franklin-graham-ukraine-war-children\",\n",
    "    \"https://www.foxnews.com/world/ukraines-foreign-minister-accuses-russia-war-crimes-attacks-school-orphanage\",\n",
    "    \"https://www.foxnews.com/world/rescuers-continue-search-kyiv-childrens-hospital-hit-russian-missile\",\n",
    "    \"https://www.foxnews.com/world/world-leaders-react-to-bucha-ukraine-massacre-russia-blamed\",\n",
    "    \"https://www.foxnews.com/world/strikes-ukraine-hospital-kills-2-day-old-baby-officials-say\",\n",
    "    \"https://www.foxnews.com/world/300-dead-mariupol-theater-russian-attacks\",\n",
    "    \"https://www.foxnews.com/world/ukraine-children-killed-wounded-russian-invasion\",\n",
    "    \"https://www.foxnews.com/world/refugees-fleeing-ukraine-reaches-2-5-million-many-children-un-says\",\n",
    "    \"https://www.foxnews.com/world/nearly-50-children-from-russian-occupied-regions-ukraine-arrive-belarus\",\n",
    "    \"https://www.foxnews.com/world/ukraine-rescue-team-brings-back-31-children-russia-war\",\n",
    "    \"https://www.foxnews.com/world/russian-attacks-on-ukrainian-hospitals-could-be-war-crimes-lawyer-says\",\n",
    "    \"https://www.foxnews.com/world/russia-denies-bombed-childrens-hospital-fake-news\",\n",
    "    \"https://www.foxnews.com/world/ukrainian-lawmaker-russia-trying-forcibly-deport-civilians-mariupol\",\n",
    "    \"https://www.bbc.com/news/articles/c62vk0v9756o\",\n",
    "    \"https://edition.cnn.com/2025/12/24/europe/moscow-bomb-police-officers-intl\",\n",
    "    \"https://edition.cnn.com/2025/12/23/europe/russian-attack-zelensky-ukraine-talks-intl\",\n",
    "    \"https://edition.cnn.com/2025/12/10/politics/ukraine-response-peace-plan\",\n",
    "    \"https://www.bbc.com/news/world-europe-60633482\",\n",
    "    \"https://amp.cnn.com/cnn/2024/06/13/europe/russia-ukraine-mariupol-hunger-war-crime-intl\",\n",
    "    \"https://amp.cnn.com/cnn/2022/03/08/europe/russia-invasion-ukraine-03-08-intl\",\n",
    "    \"https://www.theguardian.com/uk-news/2025/jun/27/ukrainians-who-fled-to-uk-being-refused-asylum-on-grounds-it-is-safe-to-return\",\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "corpus_ukraine = []\n",
    "\n",
    "# ============================\n",
    "# 2. SESSION + RETRY (TP)\n",
    "# ============================\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "}\n",
    "\n",
    "session = requests.Session()\n",
    "retry_strategy = Retry(\n",
    "    total=3,\n",
    "    backoff_factor=2,\n",
    "    status_forcelist=[429, 500, 502, 503, 504]\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "print(\"--- Début collecte corpus Ukraine ---\")\n",
    "\n",
    "# ============================\n",
    "# 3. BOUCLE DE SCRAPING\n",
    "# ============================\n",
    "for url in urls_ukraine:\n",
    "    try:\n",
    "        r = session.get(url, headers=headers, timeout=(5, 30))\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            print(f\"Erreur {r.status_code} : {url}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        # -------- TITRE --------\n",
    "        title_tag = soup.find(\"h1\")\n",
    "        titre = title_tag.get_text(strip=True) if title_tag else \"Sans titre\"\n",
    "\n",
    "        # -------- METADATA --------\n",
    "        source = \"Autre\"\n",
    "        author = \"N/A\"\n",
    "        publish_date = \"N/A\"\n",
    "        article_body = None\n",
    "\n",
    "        # -------- FOX NEWS --------\n",
    "        if \"foxnews.com\" in url:\n",
    "            source = \"Fox News\"\n",
    "            article_body = soup.find(\"div\", class_=\"article-body\")\n",
    "\n",
    "            author_tag = soup.find(\"div\", class_=\"author-byline\")\n",
    "            if author_tag:\n",
    "                author = author_tag.get_text(strip=True).replace(\"By \", \"\")\n",
    "\n",
    "            date_tag = soup.find(\"time\")\n",
    "            if date_tag:\n",
    "                publish_date = date_tag.get_text(strip=True)\n",
    "\n",
    "        # -------- BBC --------\n",
    "        elif \"bbc.com\" in url:\n",
    "            source = \"BBC\"\n",
    "            article_body = soup.find(\"article\")\n",
    "\n",
    "            date_tag = soup.find(\"time\")\n",
    "            if date_tag:\n",
    "                publish_date = date_tag.get(\"datetime\")\n",
    "\n",
    "        # -------- THE GUARDIAN --------\n",
    "        elif \"theguardian.com\" in url:\n",
    "            source = \"The Guardian\"\n",
    "            article_body = soup.find(\"div\", class_=\"article-body-commercial-selector\")\n",
    "\n",
    "            author_tag = soup.find(\"a\", rel=\"author\")\n",
    "            if author_tag:\n",
    "                author = author_tag.get_text(strip=True)\n",
    "\n",
    "            date_tag = soup.find(\"time\")\n",
    "            if date_tag:\n",
    "                publish_date = date_tag.get(\"datetime\")\n",
    "\n",
    "        # ============================\n",
    "        # 4. EXTRACTION DU TEXTE (TP)\n",
    "        # ============================\n",
    "        if article_body:\n",
    "            paragraphs = [\n",
    "                p.get_text(strip=True)\n",
    "                for p in article_body.find_all(\"p\")\n",
    "                if len(p.get_text()) > 30 and \"©\" not in p.get_text()\n",
    "            ]\n",
    "        else:\n",
    "            paragraphs = [\n",
    "                p.get_text(strip=True)\n",
    "                for p in soup.find_all(\"p\")\n",
    "                if len(p.get_text()) > 40 and \"©\" not in p.get_text()\n",
    "            ]\n",
    "\n",
    "        texte_final = \"\\n\".join(paragraphs)\n",
    "\n",
    "        # ============================\n",
    "        # 5. SAUVEGARDE\n",
    "        # ============================\n",
    "        if len(texte_final) > 150:\n",
    "            corpus_ukraine.append({\n",
    "                \"source\": source,\n",
    "                \"url\": url,\n",
    "                \"title\": titre,\n",
    "                \"publish_date\": publish_date,\n",
    "                \"author\": author,\n",
    "                \"content\": texte_final,\n",
    "                \"scraped_at\": datetime.now().isoformat(),\n",
    "                \"keywords\": [],\n",
    "                \"summary\": \"\",\n",
    "                \"conflict\": \"Ukraine\"\n",
    "            })\n",
    "            print(f\"Succès : {titre[:60]}...\")\n",
    "        else:\n",
    "            print(f\"Contenu insuffisant : {url}\")\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur critique sur {url} : {e}\")\n",
    "\n",
    "# ============================\n",
    "# 6. SAUVEGARDE JSON\n",
    "# ============================\n",
    "file_name = \"corpus_ukraine.json\"\n",
    "with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(corpus_ukraine, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n--- TERMINÉ ---\")\n",
    "print(f\"{len(corpus_ukraine)} articles sauvegardés dans {file_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f7e7d",
   "metadata": {},
   "source": [
    "**Nettoyage du texte extrait**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d232f913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total d'articles dans le corpus : 40\n",
      "\n",
      "=== RÉPARTITION PAR SOURCE (PRÉCISE) ===\n",
      "Fox News       : 32 articles (80.0%)\n",
      "CNN            : 5  articles (12.5%)\n",
      "BBC            : 2  articles (5.0%)\n",
      "The Guardian   : 1  articles (2.5%)\n",
      "\n",
      "=== ESTIMATION DES SUJETS (Précision Améliorée) ===\n",
      "Aide Humanitaire         : 9   articles ( 22.5%)\n",
      "Santé/Hôpital            : 14  articles ( 35.0%)\n",
      "Opérations Militaires    : 11  articles ( 27.5%)\n",
      "Expérience Civile        : 4   articles ( 10.0%)\n",
      "Autre                    : 2   articles (  5.0%)\n",
      "\n",
      "=== LONGUEUR DES ARTICLES ===\n",
      "Moyenne de mots par article: 670\n",
      "Article le plus court: 273 mots\n",
      "Article le plus long: 1913 mots\n",
      "\n",
      "=== SUGGESTIONS ===\n",
      "⚠️ Trop d'articles Fox News (>40%).\n",
      "✅ Taille de corpus acceptable.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# === 1. Total d’articles ===\n",
    "total_articles = len(corpus_ukraine)\n",
    "print(f\"Total d'articles dans le corpus : {total_articles}\\n\")\n",
    "\n",
    "# === 2. Répartition par source (Version Précise) ===\n",
    "# On crée une fonction pour nettoyer les noms des sources à partir des URLs\n",
    "def nettoyer_source(article):\n",
    "    src = str(article.get('source', '')).strip()\n",
    "    url = str(article.get('url', '')).lower()\n",
    "    \n",
    "    # Si la source est \"Autre\" ou mal nommée, on regarde l'URL\n",
    "    if src.lower() in ['autre', 'n/a', '', 'inconnue']:\n",
    "        if 'cnn.com' in url: return 'CNN'\n",
    "        if 'bbc.com' in url: return 'BBC'\n",
    "        if 'theguardian.com' in url: return 'The Guardian'\n",
    "        if 'foxnews.com' in url: return 'Fox News'\n",
    "        return 'Source Inconnue'\n",
    "    return src\n",
    "\n",
    "# Application du nettoyage\n",
    "sources_nettoyees = [nettoyer_source(art) for art in corpus_ukraine]\n",
    "source_counts = Counter(sources_nettoyees)\n",
    "\n",
    "print(\"=== RÉPARTITION PAR SOURCE (PRÉCISE) ===\")\n",
    "# Tri par nombre d'articles pour plus de clarté\n",
    "for source, count in source_counts.most_common():\n",
    "    pct = count / total_articles * 100\n",
    "    print(f\"{source:<15}: {count:<2} articles ({pct:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# === 3. Estimation des sujets (Corrigée pour éviter le 100%) ===\n",
    "sujet_categories_ukraine = {\n",
    "    # On retire 'support' qui est trop commun et on ajoute des mots plus précis\n",
    "    'Aide Humanitaire': ['humanitarian aid', 'unicef', 'wfp', 'food relief', 'humanitarian support', 'aid convoy'],\n",
    "    'Santé/Hôpital': ['hospital', 'medical treatment', 'patients', 'doctor', 'ambulance', 'clinic', 'surgery'],\n",
    "    'Opérations Militaires': ['missile strike', 'military operation', 'invasion', 'bombing', 'army forces', 'tanks', 'airstrike'],\n",
    "    'Expérience Civile': ['civilian casualties', 'refugees', 'displaced families', 'killed', 'wounded', 'children killed'],\n",
    "    'Autre': []\n",
    "}\n",
    "\n",
    "sujet_counts = Counter({cat: 0 for cat in sujet_categories_ukraine})\n",
    "\n",
    "for article in corpus_ukraine:\n",
    "    # On utilise le contenu pour la recherche\n",
    "    texte_complet = (article.get('title', '') + \" \" + article.get('content', '')).lower()\n",
    "    \n",
    "    counted = False\n",
    "    # On cherche des correspondances exactes pour plus de précision\n",
    "    for cat in ['Aide Humanitaire', 'Santé/Hôpital', 'Opérations Militaires', 'Expérience Civile']:\n",
    "        keywords = sujet_categories_ukraine[cat]\n",
    "        if any(word in texte_complet for word in keywords):\n",
    "            sujet_counts[cat] += 1\n",
    "            counted = True\n",
    "            break\n",
    "            \n",
    "    if not counted:\n",
    "        sujet_counts['Autre'] += 1\n",
    "\n",
    "print(\"=== ESTIMATION DES SUJETS (Précision Améliorée) ===\")\n",
    "for cat, count in sujet_counts.items():\n",
    "    pct = (count / total_articles * 100) if total_articles > 0 else 0\n",
    "    print(f\"{cat:<25}: {count:<3} articles ({pct:>5.1f}%)\")\n",
    "print()\n",
    "\n",
    "# === 4. Longueur des articles ===\n",
    "lengths = [len(article.get('content', '').split()) for article in corpus_ukraine]\n",
    "avg_len = sum(lengths) / len(lengths) if lengths else 0\n",
    "\n",
    "print(\"=== LONGUEUR DES ARTICLES ===\")\n",
    "print(f\"Moyenne de mots par article: {int(avg_len)}\")\n",
    "print(f\"Article le plus court: {min(lengths) if lengths else 0} mots\")\n",
    "print(f\"Article le plus long: {max(lengths) if lengths else 0} mots\\n\")\n",
    "\n",
    "# === 5. Suggestions ===\n",
    "print(\"=== SUGGESTIONS ===\")\n",
    "if total_articles > 0 and source_counts.get(\"Fox News\", 0) / total_articles > 0.4:\n",
    "    print(\"⚠️ Trop d'articles Fox News (>40%).\")\n",
    "print(\"✅ Taille de corpus acceptable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed7a1c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dictionnaire pour normaliser les abréviations ---\n",
    "ABBREV_MAP = {\n",
    "    r\"\\bU\\.?S\\.?\\b\": \"united_states\",\n",
    "    r\"\\bU\\.?K\\.?\\b\": \"united_kingdom\",\n",
    "    r\"\\bU\\.?N\\.?\\b\": \"united_nations\",\n",
    "    r\"\\bE\\.?U\\.?\\b\": \"european_union\",\n",
    "    r\"\\bICC\\b\": \"icc\",\n",
    "    r\"\\bICJ\\b\": \"icj\",\n",
    "    \n",
    "}\n",
    "def normalize_abbreviations(text: str) -> str:\n",
    "    text = text or \"\"\n",
    "    for pattern, repl in ABBREV_MAP.items():\n",
    "        text = re.sub(pattern, repl, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c50a386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fonction de nettoyage du texte ---\n",
    "def clean_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Mise en minuscules\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Normalisation des abréviations\n",
    "    text = normalize_abbreviations(text)\n",
    "    \n",
    "    # 3. Suppression des URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # 4. Suppression des chiffres\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 5. Suppression de la ponctuation avec regex\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # 6. Normalisation des espaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 7. Suppression des guillemets doubles\n",
    "    text = text.replace('\"', '')\n",
    "    \n",
    "    # 8. Suppression des marqueurs de métadonnées \"Source:\" et \"Context:\"\n",
    "    text = re.sub(r'\\bsource:\\b', '', text)\n",
    "    text = re.sub(r'\\bcontext:\\b', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea947180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\A\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Stopwords anglais standards\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Liste des mots à **garder** dans notre projet\n",
    "keep_words = {\n",
    "    \"we\", \"they\", \"them\", \"us\", \"our\", \"their\", \"his\", \"her\", \"its\",\n",
    "    \"may\", \"might\", \"could\", \"must\", \"should\", \"would\",\n",
    "    \"not\", \"no\", \"never\", \"without\",\n",
    "    \"by\", \"against\", \"between\", \"under\", \"over\"\n",
    "}\n",
    "\n",
    "# Supprimer ces mots des stopwords standards\n",
    "stop_words = stop_words - keep_words\n",
    "# Fonction pour supprimer les stopwords\n",
    "def remove_stopwords(text):\n",
    "    tokens = text.split()\n",
    "    tokens_clean = [t for t in tokens if t.lower() not in stop_words]\n",
    "    return \" \".join(tokens_clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c97fcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\A\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\A\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\A\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\A\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Normalisation lexicale : Stemming et Lemmatisation avec NLTK\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Initialisation\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Fonction pour convertir les tags POS de NLTK en tags WordNet\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def normalize_text_stem_lemma(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]  # A. Stemming\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(pos)) for token, pos in pos_tags]  # B. Lemmatisation\n",
    "    \n",
    "    stemmed_text = \" \".join(stemmed_tokens)\n",
    "    lemmatized_text = \" \".join(lemmatized_tokens)\n",
    "    \n",
    "    return stemmed_text, lemmatized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "13700bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus nettoyé et enrichi sauvegardé dans corpus_ukraine_clean.json\n"
     ]
    }
   ],
   "source": [
    "# --- Chargement du corpus ---\n",
    "with open('corpus_ukraine.json', 'r', encoding='utf-8') as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "# --- Application des transformations ---\n",
    "for article in corpus:\n",
    "    text = article.get(\"content\", \"\")\n",
    "    \n",
    "    # Nettoyage\n",
    "    text_clean = clean_text(text)\n",
    "    article[\"content_clean\"] = text_clean\n",
    "    \n",
    "    # Suppression stopwords\n",
    "    text_no_stop = remove_stopwords(text_clean)\n",
    "    article[\"content_no_stopwords\"] = text_no_stop\n",
    "    \n",
    "    # Stemming & Lemmatisation\n",
    "    stemmed_text, lemmatized_text = normalize_text_stem_lemma(text_no_stop)\n",
    "    article[\"content_stemmed\"] = stemmed_text\n",
    "    article[\"content_lemmatized\"] = lemmatized_text\n",
    "\n",
    "# --- Sauvegarde du corpus nettoyé ---\n",
    "with open('corpus_ukraine_clean.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(corpus, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Corpus nettoyé et enrichi sauvegardé dans corpus_ukraine_clean.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
