{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "239d4a6b",
   "metadata": {},
   "source": [
    "# pretraining \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a6a9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Pr√©traitement Expert en cours : GAZA...\n",
      "‚úÖ Termin√© ! GAZA sauvegard√© dans corpus/corpus_gaza_pretraiter.json\n",
      "   (Nettoyage Expert appliqu√©)\n",
      "\n",
      "üöÄ Pr√©traitement Expert en cours : UKRAINE...\n",
      "‚úÖ Termin√© ! UKRAINE sauvegard√© dans corpus/corpus_ukraine_pretraiter.json\n",
      "   (Nettoyage Expert appliqu√©)\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# SCRIPT DE PR√âTRAITEMENT MULTI-VUES - VERSION MASTER EXPERT\n",
    "# =================================================================\n",
    "# Auteur : Expert NLP\n",
    "# Am√©liorations :\n",
    "# 1. LISTE NOIRE EXPERTE (Supprime le bruit : said, 2024, october...)\n",
    "# 2. R√âPARATION AVANC√âE (D√©colle TrumpAnnounced, LadyMelania...)\n",
    "# 3. TROIS VUES DISTINCTES (Lexicale propre, Structurelle compl√®te, Sentiment nuanc√©)\n",
    "# =================================================================\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# --- T√âL√âCHARGEMENT DES RESSOURCES ---\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# --- 1. CONFIGURATION LINGUISTIQUE ---\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\" Convertit les tags POS pour la lemmatisation \"\"\"\n",
    "    if treebank_tag.startswith('J'): return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'): return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'): return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'): return wordnet.ADV\n",
    "    else: return wordnet.NOUN\n",
    "\n",
    "# --- 2. LISTES DE FILTRAGE ---\n",
    "\n",
    "def get_expert_stopwords():\n",
    "    \"\"\" \n",
    "    LISTE NOIRE EXPERTE : Indispensable pour Word2Vec et Nuages de Mots.\n",
    "    Retire le bruit journalistique et temporel.\n",
    "    \"\"\"\n",
    "    base_stop = set(stopwords.words('english'))\n",
    "    \n",
    "    # Bruit journalistique & Temporel √† supprimer ABSOLUMENT\n",
    "    expert_noise = {\n",
    "        # Verbes de parole\n",
    "        'said', 'told', 'reported', 'stated', 'asked', 'added', 'says', 'according', \n",
    "        'report', 'confirm', 'announced', 'claim', 'claimed',\n",
    "        # Temps\n",
    "        'year', 'month', 'day', 'today', 'yesterday', 'tuesday', 'monday', 'friday', 'sunday',\n",
    "        'october', 'november', 'december', 'january', 'february', '2023', '2024', '2025', \n",
    "        'time', 'week', 'daily', 'late', 'early', 'ago', 'since',\n",
    "        # Quantit√©s floues\n",
    "        'one', 'two', 'three', 'many', 'much', 'least', 'first', 'last', 'number', \n",
    "        'several', 'including', 'around', 'part', 'even', 'also', 'would', 'could'\n",
    "    }\n",
    "    return base_stop.union(expert_noise)\n",
    "\n",
    "def get_sentiment_stopwords():\n",
    "    \"\"\" \n",
    "    Pour le Sentiment, on GARDE les n√©gations.\n",
    "    \"\"\"\n",
    "    base_stop = set(stopwords.words('english'))\n",
    "    preserve = {'not', 'no', 'never', 'by', 'was', 'were', 'been', 'is', 'are', 'against'}\n",
    "    return base_stop - preserve\n",
    "\n",
    "# Initialisation des outils\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "STOPWORDS_LEXICAL = get_expert_stopwords()      # Pour la vue Lexicale (Stricte)\n",
    "STOPWORDS_SENTIMENT = get_sentiment_stopwords() # Pour la vue Sentiment (Nuanc√©e)\n",
    "STOPWORDS_STRUCTURAL = get_sentiment_stopwords() # Pour la vue Structurelle\n",
    "\n",
    "# --- 3. FONCTION DE R√âPARATION (CRITIQUE) ---\n",
    "def reparer_mots_colles(text):\n",
    "    \"\"\"\n",
    "    S√©pare les mots coll√©s par erreur de scraping.\n",
    "    \"\"\"\n",
    "    if not text: return \"\"\n",
    "    \n",
    "    # A. Patchs sp√©cifiques (Erreurs connues)\n",
    "    patchs = {\n",
    "        \"Trumpannounced\": \"Trump announced\",\n",
    "        \"trumpannounced\": \"trump announced\",\n",
    "        \"LadyMelania\": \"Lady Melania\",\n",
    "        \"Ladymelania\": \"Lady Melania\",\n",
    "        \"whitehouse\": \"white house\",\n",
    "        \"WhiteHouse\": \"White House\",\n",
    "        \"Fox News\": \"\", # Nettoyage source\n",
    "        \"FoxNews\": \"\"\n",
    "    }\n",
    "    for erreur, correction in patchs.items():\n",
    "        text = text.replace(erreur, correction)\n",
    "    \n",
    "    # B. Regex CamelCase (minusculeMajuscule -> minuscule Majuscule)\n",
    "    text = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', text)\n",
    "    \n",
    "    # C. Ponctuation coll√©e (mot.Majuscule -> mot. Majuscule)\n",
    "    text = re.sub(r'(?<=[a-z])\\.(?=[A-Z])', '. ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# --- 4. LES TROIS PIPELINES ---\n",
    "\n",
    "def pipeline_lexical(text):\n",
    "    \"\"\" \n",
    "    VUE 1 : LEXICALE (Pour Word2Vec, Nuages de Mots)\n",
    "    -> Nettoyage AGRESSIF (On vire 'said', '2024', etc.)\n",
    "    \"\"\"\n",
    "    text = reparer_mots_colles(text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # On ne garde que les mots alphanum√©riques, >2 lettres, et pas dans la liste experte\n",
    "    clean_tokens = [\n",
    "        t for t in tokens \n",
    "        if t.isalnum() \n",
    "        and t not in STOPWORDS_LEXICAL \n",
    "        and len(t) > 2 \n",
    "        and not t.isdigit() # On vire les nombres isol√©s (\"160\")\n",
    "    ]\n",
    "    \n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in clean_tokens]\n",
    "    return lemmas\n",
    "\n",
    "def pipeline_structural_semantique(text):\n",
    "    \"\"\" \n",
    "    VUE 2 : STRUCTURELLE (Pour S√©mantique, Syntaxe, Agence)\n",
    "    -> Garde la structure des phrases et les verbes auxiliaires\n",
    "    \"\"\"\n",
    "    text = reparer_mots_colles(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    processed_sentences = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        tokens = word_tokenize(sent)\n",
    "        tagged = pos_tag(tokens)\n",
    "        \n",
    "        sentence_data = []\n",
    "        for word, tag in tagged:\n",
    "            low_word = word.lower()\n",
    "            # Ici on filtre moins pour garder la coh√©rence grammaticale\n",
    "            if low_word not in STOPWORDS_STRUCTURAL: \n",
    "                lemma = lemmatizer.lemmatize(low_word, get_wordnet_pos(tag))\n",
    "                sentence_data.append({\"w\": word, \"t\": tag, \"l\": lemma})\n",
    "        \n",
    "        if sentence_data:\n",
    "            processed_sentences.append(sentence_data)\n",
    "            \n",
    "    return processed_sentences\n",
    "\n",
    "def pipeline_sentiment(text):\n",
    "    \"\"\" \n",
    "    VUE 3 : SENTIMENT (Pour Lab 4/9)\n",
    "    -> Garde les n√©gations ('not good')\n",
    "    \"\"\"\n",
    "    text = reparer_mots_colles(text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # On garde les mots alphanum√©riques OU les n√©gations importantes\n",
    "    clean_tokens = [\n",
    "        t for t in tokens \n",
    "        if t.isalnum() and (t not in stopwords.words('english') or t in {'not', 'no', 'never', 'against'})\n",
    "    ]\n",
    "    return clean_tokens\n",
    "\n",
    "# --- 5. EX√âCUTION ---\n",
    "\n",
    "def traiter_corpus(filename_in, filename_out, conflict_label):\n",
    "    print(f\"\\nüöÄ Pr√©traitement Expert en cours : {conflict_label}...\")\n",
    "    \n",
    "    if not os.path.exists(filename_in):\n",
    "        print(f\"‚ùå Erreur : {filename_in} introuvable.\")\n",
    "        return\n",
    "\n",
    "    with open(filename_in, 'r', encoding='utf-8') as f:\n",
    "        articles = json.load(f)\n",
    "\n",
    "    corpus_pretraite = []\n",
    "\n",
    "    for art in articles:\n",
    "        content = art['content']\n",
    "        \n",
    "        processed_data = {\n",
    "            \"title\": art.get('title', 'N/A'),\n",
    "            \"conflict\": conflict_label,\n",
    "            \"scraped_at\": art.get('scraped_at', ''),\n",
    "            \n",
    "            # VUE 1 : PROPRE (Sans bruit journalistique)\n",
    "            \"lexical_view\": pipeline_lexical(content),\n",
    "            \n",
    "            # VUE 2 : GRAMMATICALE (Avec POS tags)\n",
    "            \"structural_view\": pipeline_structural_semantique(content),\n",
    "            \n",
    "            # VUE 3 : √âMOTIONNELLE (Avec n√©gations)\n",
    "            \"sentiment_view\": pipeline_sentiment(content)\n",
    "        }\n",
    "        corpus_pretraite.append(processed_data)\n",
    "\n",
    "    with open(filename_out, 'w', encoding='utf-8') as f:\n",
    "        json.dump(corpus_pretraite, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"‚úÖ Termin√© ! {conflict_label} sauvegard√© dans {filename_out}\")\n",
    "    print(f\"   (Nettoyage Expert appliqu√©)\")\n",
    "\n",
    "# --- LANCEMENT ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Assurez-vous d'avoir vos fichiers source (v1 ou nettoye_v1)\n",
    "    traiter_corpus('corpus/corpus_palestine_nettoye_v1.json', 'corpus/corpus_gaza_pretraiter.json', 'GAZA')\n",
    "    traiter_corpus('corpus/corpus_ukraine_nettoye_v1.json', 'corpus/corpus_ukraine_pretraiter.json', 'UKRAINE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
